{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "506e0fa2",
   "metadata": {},
   "source": [
    "# Support Vector Machines-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcf5954",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataseg\n",
    "L Split the dataset into training and testing setZ\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8772490",
   "metadata": {},
   "source": [
    "# SOLUTIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805953ce",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "\n",
    "Polynomial functions and kernel functions are related in the context of machine learning, particularly in support vector machines (SVMs) and kernel methods. Kernel functions can be seen as a way to implicitly compute the dot product between data points after applying a polynomial transformation to them.\n",
    "\n",
    "In machine learning, kernel functions are used to transform data into a higher-dimensional space, making it possible to find nonlinear decision boundaries in the original feature space. A polynomial kernel is a specific type of kernel function that computes the dot product between data points in this higher-dimensional space after applying a polynomial transformation.\n",
    "\n",
    "The polynomial kernel function can be expressed as:\n",
    "\n",
    "\\[ K(x, x') = (x^T x' + c)^d \\]\n",
    "\n",
    "Here:\n",
    "- \\( x \\) and \\( x' \\) are data points.\n",
    "- \\( c \\) is a constant.\n",
    "- \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "In this way, the polynomial kernel allows SVMs to find nonlinear decision boundaries by implicitly computing the dot product in a higher-dimensional space without explicitly transforming the data. The choice of the degree \\( d \\) controls the complexity of the decision boundary, with higher degrees allowing for more complex, curved decision boundaries.\n",
    "\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "You can implement an SVM with a polynomial kernel in Python using Scikit-learn's `SVC` (Support Vector Classification) class. Here's an example:\n",
    "\n",
    "```python\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "# You can adjust the degree and C parameters as needed\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using a metric of your choice (e.g., accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- We load a dataset (Iris dataset in this case) and split it into a training set and a testing set.\n",
    "- We create an instance of the `SVC` classifier with a polynomial kernel by specifying `kernel='poly'` and setting the degree of the polynomial with the `degree` parameter.\n",
    "- We train the classifier on the training data and use it to make predictions on the testing data.\n",
    "- Finally, we evaluate the classifier's performance using accuracy, but you can use other metrics as needed.\n",
    "\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "In Support Vector Regression (SVR), epsilon (\\(\\varepsilon\\)) is a hyperparameter that determines the width of the margin around the predicted values within which no penalty is incurred. It is a crucial parameter that controls the trade-off between model complexity and the degree to which the model fits the training data.\n",
    "\n",
    "Increasing the value of epsilon (\\(\\varepsilon\\)) has the following effects on the number of support vectors:\n",
    "\n",
    "1. **More Support Vectors**: As you increase epsilon, you allow for a wider margin, which means that data points within a larger range around the predicted values are considered as support vectors. This leads to more data points being classified as support vectors.\n",
    "\n",
    "2. **Smaller Margin**: A larger epsilon implies a smaller margin around the predicted values, so the SVM tries to fit the training data more closely. This can result in a smaller margin between the predicted values and the training data, leading to a smaller margin around the support vectors.\n",
    "\n",
    "3. **Potentially Better Generalization**: A larger epsilon can make the SVR model more robust to noise in the training data and may result in better generalization to unseen data, as it allows for some tolerance for errors within the specified margin.\n",
    "\n",
    "However, it's essential to find the right balance for epsilon because excessively large values can lead to overfitting, while very small values may result in underfitting. The optimal value of epsilon depends on the specific problem and dataset.\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "- **Kernel Function**: The choice of the kernel function affects how SVR models capture nonlinear relationships in the data. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. Choose the kernel function based on the nature of the data. For example, if you suspect a nonlinear relationship, you might choose an RBF kernel.\n",
    "\n",
    "- **C Parameter**: The C parameter controls the trade-off between minimizing the error and maximizing the margin. A smaller C allows for a larger margin but may tolerate some violations of the margin. A larger C results in a smaller margin but aims to minimize training errors. Increase C when you want the model to fit the training data more closely, but be cautious of overfitting.\n",
    "\n",
    "- **Epsilon Parameter (ε)**: Epsilon determines the width of the margin around the predicted values within which no penalty is incurred. A larger epsilon allows for a wider margin and more tolerance for errors within that margin. Increase epsilon when you want the model to be more robust to noise in the training data.\n",
    "\n",
    "- **Gamma Parameter (γ)**: Gamma controls the shape and flexibility of the kernel function. A smaller gamma makes the kernel function more flexible and may lead to overfitting, especially with an RBF kernel. A larger gamma makes the kernel function more rigid and may lead to underfitting. Adjust gamma based on the complexity of the relationship you want to capture.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "- **Increasing C**: You might increase C when you suspect the data has low noise and want the SVR model to fit the training data closely, especially if there is little risk of overfitting.\n",
    "\n",
    "- **Increasing Gamma**: Increase gamma when you suspect the data has complex nonlinear relationships. However, be cautious, as a very large gamma can lead to overfitting.\n",
    "\n",
    "- **Increasing Epsilon**: You might increase epsilon when dealing with noisy data or when you want the SVR model to be more robust to outliers.\n",
    "\n",
    "It's crucial to perform hyperparameter tuning using techniques like grid search or random search to find the best combination of these parameters for your specific problem, as their optimal values can vary widely depending on the dataset and problem characteristics.\n",
    "\n",
    "Q5. Assignment: SVM Classification in Python\n",
    "\n",
    "Here is an outline of how to perform SVM classification in Python:\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X =\n",
    "\n",
    " data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling, normalization, etc.) if needed\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "# You can adjust the kernel and C parameters as needed\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using a metric of your choice (e.g., accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV\n",
    "\n",
    "# Train the tuned classifier on the entire dataset (if desired)\n",
    "svm_classifier.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(svm_classifier, 'svm_classifier.pkl')\n",
    "```\n",
    "\n",
    "This outline demonstrates the steps for SVM classification in Python, including loading the dataset, splitting it into training and testing sets, preprocessing (if necessary), training the classifier, evaluating its performance, tuning hyperparameters, and saving the trained model to a file. You can adapt it to your specific dataset and problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c749f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "#QUESTION2\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an instance of the SVC classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3, C=1.0)\n",
    "# You can adjust the degree and C parameters as needed\n",
    "\n",
    "# Train the SVM classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using a metric of your choice (e.g., accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc5e5ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##QUESTION 5\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset (e.g., Iris dataset)\n",
    "data = datasets.load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling, normalization, etc.) if needed\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svm_classifier = SVC(kernel='linear', C=1.0)\n",
    "# You can adjust the kernel and C parameters as needed\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the performance using a metric of your choice (e.g., accuracy)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV\n",
    "\n",
    "# Train the tuned classifier on the entire dataset (if desired)\n",
    "svm_classifier.fit(X, y)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(svm_classifier, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40259e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
