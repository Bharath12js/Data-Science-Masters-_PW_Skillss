{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e294f044",
   "metadata": {},
   "source": [
    "# Regression-4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9e35f",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the \n",
    "model's performance?\n",
    "\n",
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111f279",
   "metadata": {},
   "source": [
    "# SOLUTIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c80ed",
   "metadata": {},
   "source": [
    "Q1. **What is Lasso Regression, and how does it differ from other regression techniques?**\n",
    "   Lasso Regression, short for Least Absolute Shrinkage and Selection Operator, is a linear regression technique that adds a penalty term to the standard linear regression cost function. This penalty term is proportional to the absolute values of the regression coefficients. Lasso Regression is used for both prediction and feature selection in order to prevent overfitting and to encourage sparse solutions (some coefficients to be exactly zero), thereby performing automatic feature selection. Unlike ordinary linear regression, where all coefficients can take any value, Lasso tends to drive some coefficients to exactly zero, effectively eliminating those features from the model.\n",
    "\n",
    "Q2. **What is the main advantage of using Lasso Regression in feature selection?**\n",
    "   The main advantage of Lasso Regression in feature selection is that it automatically performs feature selection by driving certain coefficients to zero. This means that Lasso can effectively select the most relevant features from a larger set of features, leading to a more interpretable and potentially simpler model. This can help in reducing overfitting and improving the model's generalization ability.\n",
    "\n",
    "Q3. **How do you interpret the coefficients of a Lasso Regression model?**\n",
    "   The coefficients of a Lasso Regression model represent the effect of each feature on the target variable, just like in standard linear regression. However, due to the penalty term, some coefficients may be exactly zero. A non-zero coefficient indicates that the corresponding feature has a non-zero effect on the target variable. A coefficient being exactly zero means that the feature has been excluded from the model. The magnitude of the coefficient represents the strength of the relationship between the feature and the target, similar to ordinary linear regression.\n",
    "\n",
    "Q4. **What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?**\n",
    "   The main tuning parameter in Lasso Regression is the regularization parameter, often denoted as \"lambda\" (Î»). This parameter controls the strength of the penalty term. A larger value of lambda increases the strength of regularization, resulting in more coefficients being pushed towards zero. A smaller value of lambda reduces the impact of regularization, allowing more coefficients to have non-zero values. The choice of lambda is critical in balancing the trade-off between fitting the data well and preventing overfitting.\n",
    "\n",
    "Q5. **Can Lasso Regression be used for non-linear regression problems? If yes, how?**\n",
    "   Lasso Regression itself is a linear regression technique, which means it models linear relationships between features and the target variable. However, you can use Lasso Regression in combination with non-linear transformations of features to handle non-linear regression problems. By applying appropriate transformations (such as polynomial features or other non-linear functions) to the input features, you can make the model capture non-linear relationships. The transformed features are then used in the Lasso Regression model.\n",
    "\n",
    "Q6. **What is the difference between Ridge Regression and Lasso Regression?**\n",
    "   Ridge Regression and Lasso Regression are both regularization techniques, but they use different penalty terms. Ridge Regression adds a penalty term proportional to the square of the coefficients (L2 penalty), while Lasso Regression adds a penalty term proportional to the absolute value of the coefficients (L1 penalty). Because of this, Ridge tends to shrink coefficients towards zero but rarely makes them exactly zero, whereas Lasso can drive coefficients exactly to zero, effectively performing feature selection. This makes Lasso more suitable when you believe that many features are irrelevant or redundant.\n",
    "\n",
    "Q7. **Can Lasso Regression handle multicollinearity in the input features? If yes, how?**\n",
    "   Yes, Lasso Regression can handle multicollinearity to some extent. Multicollinearity refers to the presence of high correlation between predictor variables. Lasso's feature selection mechanism can automatically select one feature over another if they are highly correlated, effectively reducing the impact of multicollinearity by setting one of the correlated coefficients to zero. However, if multicollinearity is severe, it might still lead to instability in the coefficient estimates.\n",
    "\n",
    "Q8. **How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**\n",
    "   The choice of the regularization parameter lambda is crucial and is often determined through techniques like cross-validation. In k-fold cross-validation, the dataset is divided into k subsets, and the model is trained and validated multiple times with different subsets used for validation each time. The value of lambda that results in the best validation performance (e.g., lowest mean squared error) across the different folds is typically chosen as the optimal lambda. Grid search or more sophisticated techniques like coordinate descent can also be used to search for the optimal lambda value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21c4c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
