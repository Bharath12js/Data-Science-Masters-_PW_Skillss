{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6804c6e",
   "metadata": {},
   "source": [
    "# Boosting-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dbf7d5",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b46a66",
   "metadata": {},
   "source": [
    "# SOLUTIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beac50b3",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "   Gradient Boosting Regression is a machine learning algorithm that belongs to the ensemble learning family. It is primarily used for regression tasks, where the goal is to predict a continuous numerical value. Gradient Boosting Regression builds an ensemble of decision trees sequentially, with each tree correcting the errors of the previous ones. The algorithm minimizes a loss function (typically a mean squared error) by adjusting the predictions iteratively. Gradient Boosting is known for its high predictive accuracy and is used in various applications, including predictive modeling and forecasting.\n",
    "\n",
    "Q2. Implementing Gradient Boosting Regression from Scratch in Python:\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Generate a simple dataset for regression\n",
    "np.random.seed(0)\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.rand(80)\n",
    "\n",
    "# Define a Gradient Boosting Regressor class\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        residuals = y.copy()\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.models.append(tree)\n",
    "            predictions = tree.predict(X)\n",
    "            residuals -= self.learning_rate * predictions\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(len(X))\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "```\n",
    "\n",
    "This code defines a basic Gradient Boosting Regressor class, trains it on a synthetic dataset, and evaluates its performance using mean squared error (MSE) and R-squared (R2) metrics.\n",
    "\n",
    "Q3. Experimenting with Hyperparameters:\n",
    "   To optimize the model's performance, you can experiment with different hyperparameters such as learning rate, the number of trees (n_estimators), and tree depth (max_depth). You can use grid search or random search techniques along with cross-validation to find the best hyperparameters that minimize the validation error.\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "   A weak learner in Gradient Boosting is a simple and relatively low-performing model that is used as a base learner in the ensemble. Weak learners are typically shallow decision trees (also known as decision stumps) or linear models. They are individually not very accurate but are sequentially combined and weighted to form a strong ensemble model that can make accurate predictions.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "   The intuition behind Gradient Boosting is to iteratively correct the errors made by the previous models in the ensemble. It works by fitting a weak learner to the residuals (the differences between the actual target values and the predictions made by the current ensemble). In each iteration, the weak learner focuses on the samples where the ensemble's predictions are the least accurate. This process continues until the ensemble's predictions become increasingly accurate, resulting in a strong learner.\n",
    "\n",
    "Q6. How does the Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "   The Gradient Boosting algorithm builds an ensemble of weak learners sequentially. The key steps are as follows:\n",
    "   1. Initialize the ensemble with a simple model (e.g., a decision stump) or a constant value.\n",
    "   2. Calculate the residuals by subtracting the current ensemble's predictions from the actual target values.\n",
    "   3. Fit a weak learner (e.g., decision stump) to the residuals to capture the errors made by the ensemble.\n",
    "   4. Update the ensemble by adding the weak learner with a scaled weight (learning rate).\n",
    "   5. Repeat steps 2-4 until a predefined number of iterations (n_estimators) is reached.\n",
    "\n",
    "   Each weak learner focuses on the patterns that the ensemble has not captured well, gradually improving the overall predictive accuracy.\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of the Gradient Boosting algorithm?\n",
    "   The mathematical intuition of Gradient Boosting involves the following steps:\n",
    "   1. Initialize the ensemble's predictions with a constant value (e.g., the mean of the target values).\n",
    "   2. Calculate the residuals (errors) by subtracting the current ensemble's predictions from the actual target values.\n",
    "   3. Train a weak learner (e.g., decision stump) to fit the residuals, essentially finding patterns in the errors.\n",
    "   4. Update the ensemble by adding the weak learner's predictions, scaled by a learning rate, to the current predictions.\n",
    "   5. Repeat steps 2-4 iteratively, where each weak learner focuses on the remaining errors and adds to the ensemble's predictive power.\n",
    "   6. The final ensemble is a combination of all weak learners, and its predictions become increasingly accurate as more learners are added.\n",
    "\n",
    "   The algorithm minimizes a loss function (typically mean squared error for regression) during this process, driving the predictions to approach the true target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc2da04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
