{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94e42935",
   "metadata": {},
   "source": [
    "# Assignment - CNN Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e04a3",
   "metadata": {},
   "source": [
    "### Understanding Pooling and Padding in CNN:\n",
    "\n",
    "**Purpose and Benefits of Pooling:**\n",
    "Pooling in Convolutional Neural Networks (CNNs) serves the purpose of downsampling or reducing the spatial dimensions of the input features. It helps in extracting the most essential information while reducing computational complexity. The benefits include:\n",
    "- **Dimensionality Reduction:** Pooling reduces the size of the feature maps, decreasing the number of parameters and computations in subsequent layers.\n",
    "- **Translation Invariance:** Pooling helps make the CNN more robust to variations in the position of features, providing some degree of translation invariance.\n",
    "- **Feature Generalization:** Pooling captures the most important features, aiding in generalization and preventing overfitting.\n",
    "\n",
    "**Difference between Max Pooling and Average Pooling:**\n",
    "- **Max Pooling:** Takes the maximum value from the pool, emphasizing the most activated features. Suitable for emphasizing prominent features.\n",
    "- **Average Pooling:** Computes the average value, providing a more smoothed representation. Useful when preserving general patterns is more critical.\n",
    "\n",
    "### Exploring LeNet-5:\n",
    "\n",
    "**Overview of LeNet-5 Architecture:**\n",
    "LeNet-5 is a pioneering convolutional neural network architecture designed for handwritten digit recognition. It consists of the following key components:\n",
    "- **Convolutional Layers:** Process input images using learnable filters.\n",
    "- **Pooling Layers:** Downsample feature maps to reduce spatial dimensions.\n",
    "- **Fully Connected Layers:** Process flattened features for final classification.\n",
    "- **Activation Functions:** Typically use tanh or sigmoid activation functions.\n",
    "- **Gradient-Based Learning:** Trained using gradient-based optimization algorithms like stochastic gradient descent.\n",
    "\n",
    "**Advantages and Limitations of LeNet-5:**\n",
    "- **Advantages:**\n",
    "  - Effective for handwritten digit recognition.\n",
    "  - Introduced the concept of convolutional neural networks.\n",
    "- **Limitations:**\n",
    "  - Limited scalability for larger and more complex datasets.\n",
    "  - May struggle with deeper and more challenging tasks compared to modern architectures.\n",
    "\n",
    "**Implementation of LeNet-5:**\n",
    "Implementation of LeNet-5 involves building the architecture using a deep learning framework (e.g., TensorFlow, PyTorch) and training it on a dataset such as MNIST. Evaluation includes assessing its performance metrics and providing insights into its strengths and weaknesses.\n",
    "\n",
    "### Analyzing AlexNet:\n",
    "\n",
    "**Overview of AlexNet Architecture:**\n",
    "AlexNet is a groundbreaking CNN architecture designed for the ImageNet Large Scale Visual Recognition Challenge. Key components include:\n",
    "- **Convolutional Layers:** Employ large receptive fields and multiple filters.\n",
    "- **Local Response Normalization (LRN):** Normalize neuron responses to enhance generalization.\n",
    "- **Pooling Layers:** Downsample feature maps.\n",
    "- **Fully Connected Layers:** Process flattened features for classification.\n",
    "- **Dropout:** Regularization technique to prevent overfitting.\n",
    "\n",
    "**Architectural Innovations and Breakthrough Performance:**\n",
    "- **Parallel Processing:** Splitting the network into two GPU-friendly streams.\n",
    "- **ReLU Activation:** Faster convergence due to faster learning.\n",
    "- **Data Augmentation:** Increase dataset size by applying transformations during training.\n",
    "\n",
    "**Implementation and Evaluation:**\n",
    "Implementation of AlexNet involves using a deep learning framework, training it on a chosen dataset, and evaluating its performance metrics. Insights are derived from understanding how well the model performs on the given task and its computational efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe2df5b",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape((60000, 28, 28, 1)).astype('float32') / 255\n",
    "test_images = test_images.reshape((10000, 28, 28, 1)).astype('float32') / 255\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Build LeNet-5 architecture\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(16, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(120, activation='relu'))\n",
    "model.add(layers.Dense(84, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_images, train_labels, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f'Test accuracy: {test_acc}')\n",
    "```\n",
    "\n",
    "This code defines the LeNet-5 architecture with two convolutional layers, max-pooling layers, and fully connected layers. It uses the MNIST dataset, preprocesses the data, compiles the model, trains it, and then evaluates its performance on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640826c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
