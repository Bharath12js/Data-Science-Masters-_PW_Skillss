{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aac01ea",
   "metadata": {},
   "source": [
    "# KNN Assignment - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6280114",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5343774",
   "metadata": {},
   "source": [
    "# SOLUTIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3b9462",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "   The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It makes predictions by finding the K training examples (data points) in the training dataset that are closest to a given input data point and then classifying or regressing based on the majority class or averaging the target values of those K neighbors.\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "   The choice of the value of K in KNN can significantly impact the algorithm's performance. Selecting the right K value depends on the dataset and the problem at hand. Here are some common approaches:\n",
    "   - **Odd K Values:** Use odd values for K to avoid ties when determining the majority class.\n",
    "   - **Cross-Validation:** Perform cross-validation with different K values and choose the one that yields the best performance on a validation set.\n",
    "   - **Square Root of N:** A heuristic often used is to set K to the square root of the number of data points (N) in the training dataset.\n",
    "   - **Domain Knowledge:** Consider domain-specific knowledge or insights when choosing K.\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "   - **KNN Classifier:** KNN Classifier is used for classification tasks. It assigns a data point to the majority class among its K nearest neighbors.\n",
    "   - **KNN Regressor:** KNN Regressor is used for regression tasks. It calculates the average (or weighted average) of the target values of its K nearest neighbors and uses this value as the prediction.\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "   The performance of KNN can be measured using various metrics depending on whether it's a classification or regression task. Common metrics include:\n",
    "   - **Classification Metrics:** Accuracy, Precision, Recall, F1-score, ROC-AUC, etc.\n",
    "   - **Regression Metrics:** Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), R-squared (R2), etc.\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "   The curse of dimensionality refers to the challenge that arises in high-dimensional spaces, where the volume of the space grows exponentially with the number of dimensions. In KNN, this can lead to several issues, such as:\n",
    "   - Increased computational complexity.\n",
    "   - Increased distance between data points, making it harder to find nearest neighbors accurately.\n",
    "   - Diminishing effectiveness of distance-based similarity measures.\n",
    "   To mitigate the curse of dimensionality, feature selection, dimensionality reduction techniques (e.g., PCA), or using distance metrics designed for high-dimensional spaces can be employed.\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "   Handling missing values in KNN can be challenging. Some approaches include:\n",
    "   - Imputation: Fill missing values with estimates (e.g., mean, median, or mode) based on the non-missing neighbors.\n",
    "   - Deletion: Remove data points with missing values (if not too many).\n",
    "   - Custom Distance Metrics: Modify the distance metric to handle missing values by assigning a penalty or using specific imputation techniques.\n",
    "   The choice depends on the specific problem and the amount of missing data.\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "   - **KNN Classifier:** Suitable for classification tasks where the target variable is categorical. It assigns data points to classes based on the majority class among its neighbors. Works well when the decision boundary is relatively simple.\n",
    "   - **KNN Regressor:** Appropriate for regression tasks where the target variable is continuous. It estimates the target value based on the average of its neighbors' values. Works well when there is a continuous relationship between features and the target.\n",
    "   The choice between KNN Classifier and Regressor depends on the nature of the problem and the type of target variable.\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "   **Strengths**:\n",
    "   - Simple to implement and understand.\n",
    "   - Can handle multi-class classification.\n",
    "   - Non-parametric and can capture complex relationships.\n",
    "   - Works well when data is not linearly separable.\n",
    "\n",
    "   **Weaknesses**:\n",
    "   - Sensitive to outliers and noisy data.\n",
    "   - Computationally expensive for large datasets and high dimensions.\n",
    "   - Choice of K is critical and can affect results.\n",
    "   - Can suffer from the curse of dimensionality.\n",
    "   \n",
    "   Addressing these weaknesses may involve preprocessing (outlier handling, dimensionality reduction), selecting an appropriate distance metric, and tuning hyperparameters.\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "   - **Euclidean Distance:** Euclidean distance measures the \"as-the-crow-flies\" or straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of squared differences between corresponding elements of the two vectors.\n",
    "   - **Manhattan Distance:** Manhattan distance (also known as L1 distance or taxicab distance) calculates the distance as the sum of the absolute differences between corresponding elements of the two vectors. It is the distance traveled along gridlines in a city grid.\n",
    "\n",
    "   The choice between Euclidean and Manhattan distance depends on the problem and the nature of the data. Manhattan distance is often preferred when movement can only occur along gridlines (e.g., in grid-based maps), while Euclidean distance is used in continuous spaces.\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?\n",
    "    Feature scaling in KNN ensures that all features contribute equally to the distance calculation between data points. Without feature scaling, features with larger scales may dominate the distance calculation. Common scaling methods include Min-Max scaling (scaling features to a specific range) and Standardization (scaling to have mean 0 and standard deviation 1). Feature scaling helps KNN perform more effectively and avoids bias due to differences in feature scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b946a5f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
