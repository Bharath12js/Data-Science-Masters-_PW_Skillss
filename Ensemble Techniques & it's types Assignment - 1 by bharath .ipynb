{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c22b0c0",
   "metadata": {},
   "source": [
    "# Ensemble Techniques & it's types Assignment - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f819bf",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Q3. What is bagging?\n",
    "\n",
    "Q4. What is boosting?\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc30214c",
   "metadata": {},
   "source": [
    "# Solutions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb463c",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "   An ensemble technique in machine learning involves combining multiple individual models to create a single, more robust and accurate predictive model. The idea behind ensemble techniques is to leverage the diversity of different models to improve overall prediction performance.\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "   Ensemble techniques are used in machine learning for several reasons:\n",
    "   - They can reduce overfitting: Combining multiple models helps reduce the risk of overfitting by taking into account different patterns in the data.\n",
    "   - They improve prediction accuracy: Ensembles often outperform individual models, especially when the individual models have complementary strengths and weaknesses.\n",
    "   - They enhance model robustness: Ensembles are less sensitive to noise and outliers in the data.\n",
    "   - They can handle complex relationships: Ensembles can capture complex relationships in the data by combining simpler models.\n",
    "   - They are versatile: Ensemble methods can be applied to various types of machine learning algorithms, such as decision trees, neural networks, and more.\n",
    "\n",
    "Q3. What is bagging?\n",
    "   Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in which multiple bootstrap samples (randomly sampled subsets with replacement) of the training data are used to train multiple base models independently. These base models can be of the same type (e.g., decision trees) or different types. Bagging reduces variance and improves model stability by combining the predictions of these base models, often by averaging (for regression) or voting (for classification).\n",
    "\n",
    "Q4. What is boosting?\n",
    "   Boosting is another ensemble technique that combines multiple weak learners (typically shallow or simple models) to create a strong learner. Unlike bagging, boosting trains base models sequentially, with each new model focusing on the data points that previous models struggled with. Boosting assigns weights to data points and updates them during training to give more importance to misclassified instances. The final prediction is a weighted combination of the base models. Popular boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "   The benefits of using ensemble techniques include:\n",
    "   - Improved prediction accuracy.\n",
    "   - Reduced overfitting and increased model generalization.\n",
    "   - Robustness to noisy data and outliers.\n",
    "   - Effective handling of complex relationships in data.\n",
    "   - Versatility, as ensemble methods can be applied to various algorithms.\n",
    "   - Enhanced model stability.\n",
    "   - Increased interpretability in some cases (e.g., feature importance in tree-based ensembles).\n",
    "\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "   Ensemble techniques are not always better than individual models. Their performance depends on factors such as the quality of base models, the diversity of those models, and the nature of the data. In some cases, a well-tuned individual model may outperform an ensemble. However, ensembles are generally preferred because they tend to provide better overall performance and are more robust, especially when dealing with complex or noisy data.\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "   To calculate a confidence interval using bootstrap, you perform the following steps:\n",
    "   1. Create multiple (typically thousands) bootstrap samples by randomly selecting data points from the original dataset with replacement.\n",
    "   2. For each bootstrap sample, compute the statistic of interest (e.g., mean, median).\n",
    "   3. Calculate the desired percentile of the distribution of these statistics to obtain the lower and upper bounds of the confidence interval.\n",
    "\n",
    "Q8. How does bootstrap work, and what are the steps involved in bootstrap?\n",
    "   Bootstrap is a resampling technique used for estimating the sampling distribution of a statistic or making inferences about a population. The steps involved in bootstrap are as follows:\n",
    "   1. **Sample with Replacement**: Randomly select data points from the original dataset, allowing for duplicates (sampling with replacement) to create a bootstrap sample of the same size as the original data.\n",
    "   2. **Calculate Statistic**: Compute the statistic of interest (e.g., mean, median, variance) on the bootstrap sample.\n",
    "   3. **Repeat**: Repeat steps 1 and 2 a large number of times (typically thousands) to generate a distribution of the statistic.\n",
    "   4. **Analyze Distribution**: Analyze the distribution of the statistic to make inferences, such as estimating confidence intervals or assessing the uncertainty associated with the statistic.\n",
    "\n",
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.\n",
    "   \n",
    "   To estimate the 95% confidence interval for the population mean height using bootstrap, follow these steps:\n",
    "\n",
    "   1. Create a large number of bootstrap samples (e.g., 10,000) by randomly selecting 50 heights from the sample with replacement.\n",
    "   2. Calculate the mean for each bootstrap sample.\n",
    "   3. Sort the bootstrap sample means in ascending order.\n",
    "   4. Find the 2.5th percentile and the 97.5th percentile of the sorted means to obtain the lower and upper bounds of the 95% confidence interval.\n",
    "\n",
    "   Here's how you can calculate it in Python (assuming you have a list of heights called `heights`):\n",
    "\n",
    "   ```python\n",
    "   import numpy as np\n",
    "\n",
    "   # Number of bootstrap samples\n",
    "   num_samples = 10000\n",
    "\n",
    "   # Initialize an array to store bootstrap sample means\n",
    "   bootstrap_means = []\n",
    "\n",
    "   # Perform bootstrapping\n",
    "   for _ in range(num_samples):\n",
    "       # Randomly sample with replacement from the heights\n",
    "       bootstrap_sample = np.random.choice(heights, size=50, replace=True)\n",
    "       # Calculate the mean of the bootstrap sample\n",
    "       bootstrap_mean = np.mean(bootstrap_sample)\n",
    "       bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "   # Calculate the 95% confidence interval\n",
    "   lower_bound = np.percentile(bootstrap_means, 2.5)\n",
    "   upper_bound = np.percentile(bootstrap_means, 97.5)\n",
    "\n",
    "   print(f\"95% Confidence Interval: ({lower_bound:.2f}, {upper_bound:.2f}) meters\")\n",
    "   ```\n",
    "\n",
    "   This code will give you the estimated 95% confidence interval for the population mean height based on the bootstrap resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde2f29",
   "metadata": {},
   "source": [
    "# ------------------------------------------------END-----------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
